# Markov Decision Processes
---
> [!info]- File Details
> Includes information about this (genus:: Note) from [Year::3]. Contains details on when this was created, what module the note belongs to.
> > *Date :*  15-10-2025
> > *Module :* [[Reinforcement Learning]]
> > *Teacher*: 
> > *Resources :*

---
> [!abstract]+ Contents
> List of headings within this topic
> > [[#Speed run]]
> [[#]]


--- 
> [!danger]+ ðŸ•°ï¸* Speed run*
> Break down of topic 
> - The **Markov property** says:
> 	â€œThe future depends only on the **current state and action**, not on what happened before.â€
> 
> - Functions best when no model given. Learns from taking actions
> - episodic means can be split in episodes
> - continuation task mean the task has no clear breaks/ last forever
> - introduced notation with which we can describe both episodic and continuing sequential problems,
> - discussed what information needs to be summarised in our state representation, in order to ensure that theÂ **Markov property**Â holds, and
> - formulated sequential decision problems formally asÂ **Markov decision processes (MDPs)**.

---


1. Components of MDP: states, actions, transition probabilities, rewards, and initial state distribution,
â€¢ A set of **states**: $S$
â€¢ A set of **actions** available in each state: $A(s), s \in S$
â€¢ A **transition** function: $p(s'|s, a), s \in S, s' \in S, a \in A(s)$
â€¢ A **reward** function: $r(s, a, s'), s \in S, s' \in S, a \in A(s)$
â€¢ An **initial** state distribution: $h(s), s \in S$
We will often combine $p$ and $r$ into $p(s', r | s, a).$

2. Value functions: state value function and action value function,
#todo 
3. Optimal policies and comparison of policies,
4. Bellman equations: Bellman equation for state value and Bellman optimality equation,

5. Dynamic programming methods for solving MDPs,
![[Dynamic Prog for Bellman.png|400]]
7. Policy evaluation and policy improvement,
8. Iterative policy evaluation algorithm,
![[Pasted image 20251015191745.png|500]]
9. Policy improvement process and general policy iteration,
10. Value iteration algorithm,
11. Summary of dynamic programming methods' limitations,
12. Introduction to Monte Carlo methods (upcoming content next week),
13. Recap of key points from the lecture

