# Temp Reinf Learning Cheat Sheet
---
> [!info]+ Module Details
> Includes information about (genus:: Cheat Sheet) from [Year::3]. Links to Module Note and it's correspondent attribute tag 
> *Module Tag :* 
> *Link :* 
> *Cheat Sheet tag :* [[Grail ðŸ©·]]
> 


> [!abstract]+ Contents
> 
> > [[#ðŸ•°ï¸ Speed run]]  
> > [[#Exam breakdown]]
> > [[#ðŸ§  Definitions by Topic]]

---

> [!danger]+ _ðŸ•°ï¸ Speed run_
> 
> - **Topic**: Keyterm 1, Keyterm 2, Keyterm 3
>  
> - **Topic**: Key terms
> 
> - **Topic**: Key terms

---
# Exam breakdown

> [!danger]+ Exam
> Exam details when given
> 
> 
> > [!heart ]+ Structure Overview
> > E.G MCQ w 1 long answer worth 20 marks (probably x)
> 
> > [!omega]+ KEY NOTE
> > Important details stated

---
# ðŸ§  Definitions by Topic

> [!example]+ Models
> > [!leaf]- Dynamic Programming 
> > Update values  of a state based on **ALL** outcomes of immediate reward and next state 
> > Requires model $p(p`,r|s,a)$
> > ![[Dynamic Prog visual.png|400]]
> 
> > [!heart]- Monte Carlo 
> > We update the value of a state based on full sample return generated by out agent after starting in the state
> > Reqs **Full episodes** can only be used on episodic
> > 
> > Transition Functions, Policy Evaluation, Expected Return, First-Visit Monte Carlo, Every-Visit Monte Carlo, Exploring Starts, Epsilon-Greedy, Off-Policy Learning, On-Policy Learning, Behavior Policy, Target Policy, Importance Sampling, Coverage Assumption, Value Estimates, Average Returns, Action Selection, Exploration, Exploitation, Soft policy
> > ![[Monte Carlo Visual.png|400]]
> 
> > [!abstract]- Temporal Difference 
> > Takes inspiration from Bellman 
> > Upd val of state using single time-step of experience
> > THESE TAKE INTO ACCOUNT THE SEQUENTIAL NATURE OF LIFE
> > 
> > Bellman Equation, Value Function, Bootstrapping, Epsilon-Greedy, Control Problem, Prediction Problem, First-Visit Monte Carlo, Off-Policy, On-Policy, Importance Sampling, Optimal Policy, TD Error, Exploration, Exploitation, A/B Testing, Q-Learning, Learning Rate, Discount Factor, Behavior Policy, Target Policy, Neural Networks.
> > ![[TD Prediction Visual.png|400]]
> > This update can be re-arragned to give the **TD Error** which minimises error. This is the block of immediate reward + discounted next reward - old expected reward
> > ![[Pasted image 20251029183613.png|300]]
> > > [!Success] Variants 
> > > SARSA is on-policy TD 
> > > QLEarning is Off-policy TD
> > 
> > 
> 
> > [!tip]+ Overview of the 3
> > ![[Summary of 1st Half of Sem 1 RL.png|300]]
> > **Bootstrapping**: Do we base our new estimates off of old estimates?
> > **Sampling**: Can we update based on generated sample experience instead of relying on a perfect model ?
> > ![[Table of Bootstrapping & Sampling.png|300]]
> 
> 
> > [!NOTE]-  Dyna Q
> >  Q learning, but after each update, take random **simulated** steps that help propagate the updated value. However, if environment changes, its cooked. New exploit is easier to learn than old best path blocked. Dyna Q + solves changing environment
> > 
> > > [!NOTE] Dyna Q + 
> > >  the longer it has been since we tried $(S, a)$ assume higher chance it has changed.  $\tau_{s,a}$ is the time steps since reward  taken. This $\gamma$ addition rewards exploration, 
> > >  $Q(s,a) \leftarrow Q(s,a) + \alpha [ r + k\sqrt{\tau_{s,a}} + \gamma \max_{a'} Q(s',a') - Q(s,a) ]$



---
#TODO
[[Grail ðŸ©·]]

