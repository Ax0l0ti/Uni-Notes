# Monte Carlo Methods
---
> [!info]- File Details
> Includes information about this (genus:: Note) from [Year::3]. Contains details on when this was created, what module the note belongs to.
> > *Date :*  15-10-2025
> > *Module :* [[Reinforcement Learning]]
> > *Teacher*: 
> > *Resources :*

---
> [!abstract]+ Contents
> List of headings within this topic
> > [[#Speed run]]
> [[#]]


--- 
> [!danger]+ ðŸ•°ï¸* Speed run*
> Break down of topic 
> **Monte Carlo**  : Value of $V(S) = \text{Average of all returns after visiting S}$
> 
> In this lesson, we introducedÂ **Monte Carlo (MC)**Â methods, a class of solution methods that learn based on full episodes of experience generated by our agents. Over the course of the lesson, we built up our understanding of how Monte Carlo methods work, and concluded by examining a concrete Monte Carlo algorithm for policy evaluation. 
> - **Don't require access to a perfect world model**,
> - **Learn solely from full episodes of agent experience**Â andÂ **sample returns**,
> - how Monte Carlo updatesÂ **do not bootstrap**,
> - that, when computing sample returns, we can take either aÂ **first-visit**Â orÂ **every-visit**Â approach, and
> - how to implement theÂ **first-visit Monte Carlo prediction**Â algorithm.
>

---

#TODO 