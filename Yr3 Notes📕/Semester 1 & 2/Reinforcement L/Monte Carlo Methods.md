# Monte Carlo Methods
---
> [!info]- File Details
> Includes information about this (genus:: Note) from [Year::3]. Contains details on when this was created, what module the note belongs to.
> > *Date :*  15-10-2025
> > *Module :* [[Reinforcement Learning]]
> > *Teacher*: 
> > *Resources :*

---
> [!abstract]+ Contents
> List of headings within this topic
> > [[#Speed run]]
> [[#]]


--- 
> [!danger]+ ðŸ•°ï¸* Speed run*
> Break down of topic 
> In this lesson, we introducedÂ **Monte Carlo (MC)**Â methods, a class of solution methods that learn based on full episodes of experience generated by our agents. Over the course of the lesson, we built up our understanding of how Monte Carlo methods work, and concluded by examining a concrete Monte Carlo algorithm for policy evaluation. 
> - how, unlike dynamic programming methods,Â **Monte Carlo methods don't require access to a perfect world model**,
> - how Monte Carlo methodsÂ **learn solely from full episodes of agent experience**Â andÂ **sample returns**,
> - how Monte Carlo updatesÂ **do not bootstrap**,
> - that, when computing sample returns, we can take either aÂ **first-visit**Â orÂ **every-visit**Â approach, and
> - how to implement theÂ **first-visit Monte Carlo prediction**Â algorithm.
>

---

#TODO 